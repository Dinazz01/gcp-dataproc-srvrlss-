modules/dataproc-serverless/versions.tf

terraform {
  required_version = ">= 1.0"
  
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = ">= 4.0"
    }
    google-beta = {
      source  = "hashicorp/google-beta"
      version = ">= 4.0"
    }
  }
}
modules/dataproc-serverless/variables.tf
hcl
# Required Variables
variable "project_id" {
  description = "The GCP project ID"
  type        = string
}

variable "region" {
  description = "The GCP region for resources"
  type        = string
  default     = "us-central1"
}

variable "batch_id" {
  description = "Unique ID for the Dataproc Serverless batch job"
  type        = string
}

# Spark Job Configuration
variable "spark_job_config" {
  description = "Configuration for the Spark job"
  type = object({
    main_jar_file_uri  = optional(string)
    main_class         = optional(string)
    main_python_file_uri = optional(string)
    jar_file_uris      = optional(list(string), [])
    python_file_uris   = optional(list(string), [])
    file_uris          = optional(list(string), [])
    archive_uris       = optional(list(string), [])
    args               = optional(list(string), [])
  })
  default = {}
}

# Runtime Configuration
variable "runtime_config" {
  description = "Runtime configuration for the Spark job"
  type = object({
    version             = optional(string, "2.1") # Dataproc version
    container_image     = optional(string)
    properties          = optional(map(string), {})
    logging_config = optional(object({
      driver_log_levels = optional(map(string), {})
    }), {})
  })
  default = {}
}

# Environment Configuration
variable "environment_config" {
  description = "Environment configuration for the batch job"
  type = object({
    execution_config = optional(object({
      service_account        = optional(string)
      subnetwork_uri         = optional(string)
      network_tags           = optional(list(string), [])
      kms_key                = optional(string)
      ttl                    = optional(string) # "3600s" format
      staging_bucket         = optional(string)
    }), {})
    peripherals_config = optional(object({
      metastore_service = optional(string)
      spark_history_server_config = optional(object({
        dataproc_cluster = optional(string)
      }))
    }), {})
  })
  default = {}
}

# IAM Configuration
variable "service_account_email" {
  description = "Service account email to run the batch job"
  type        = string
  default     = null
}

# Network Configuration
variable "network_config" {
  description = "Network configuration"
  type = object({
    network_name    = optional(string)
    subnetwork_name = optional(string)
  })
  default = {}
}

# Common Labels
variable "labels" {
  description = "Labels to apply to resources"
  type        = map(string)
  default     = {}
}

# Timeouts
variable "timeouts" {
  description = "Timeouts for batch job creation"
  type = object({
    create = optional(string, "30m")
    update = optional(string, "30m")
    delete = optional(string, "30m")
  })
  default = {}
}
modules/dataproc-serverless/main.tf
hcl
# Enable required APIs
resource "google_project_service" "apis" {
  for_each = toset([
    "dataproc.googleapis.com",
    "compute.googleapis.com",
    "storage.googleapis.com",
    "bigquery.googleapis.com",
    "bigquerystorage.googleapis.com",
    "logging.googleapis.com",
    "monitoring.googleapis.com"
  ])

  project = var.project_id
  service = each.key

  disable_dependent_services = true
  disable_on_destroy         = false
}

# Create a service account for Dataproc if not provided
resource "google_service_account" "dataproc_sa" {
  count = var.service_account_email == null ? 1 : 0

  project      = var.project_id
  account_id   = "dataproc-serverless-sa"
  display_name = "Dataproc Serverless Service Account"
  description  = "Service account for Dataproc Serverless batch jobs"

  depends_on = [google_project_service.apis]
}

# Add IAM roles to service account
resource "google_project_iam_member" "dataproc_roles" {
  for_each = toset([
    "roles/dataproc.editor",
    "roles/storage.objectAdmin",
    "roles/bigquery.dataEditor",
    "roles/bigquery.jobUser",
    "roles/logging.logWriter",
    "roles/monitoring.metricWriter"
  ])

  project = var.project_id
  role    = each.key
  member  = var.service_account_email != null ? "serviceAccount:${var.service_account_email}" : "serviceAccount:${google_service_account.dataproc_sa[0].email}"

  depends_on = [google_service_account.dataproc_sa]
}

# Get network information if provided
data "google_compute_network" "default" {
  count = try(var.network_config.network_name, null) != null ? 1 : 0

  project = var.project_id
  name    = var.network_config.network_name
}

data "google_compute_subnetwork" "default" {
  count = try(var.network_config.subnetwork_name, null) != null ? 1 : 0

  project = var.project_id
  name    = var.network_config.subnetwork_name
  region  = var.region
}

# Create staging bucket if not provided
resource "google_storage_bucket" "staging_bucket" {
  count = try(var.environment_config.execution_config.staging_bucket, null) == null ? 1 : 0

  project                     = var.project_id
  name                        = "${var.project_id}-dataproc-staging-${random_id.bucket_suffix[0].hex}"
  location                    = var.region
  force_destroy               = true
  uniform_bucket_level_access = true

  labels = var.labels

  depends_on = [google_project_service.apis]
}

resource "random_id" "bucket_suffix" {
  count = try(var.environment_config.execution_config.staging_bucket, null) == null ? 1 : 0

  byte_length = 8
}

# Main Dataproc Serverless Batch resource
resource "google_dataproc_batch" "spark_batch" {
  provider = google-beta

  project   = var.project_id
  region    = var.region
  batch_id  = var.batch_id
  
  labels = var.labels

  # Spark Configuration
  dynamic "spark_batch" {
    for_each = [var.spark_job_config]
    content {
      main_jar_file_uri  = spark_batch.value.main_jar_file_uri
      main_class         = spark_batch.value.main_class
      main_python_file_uri = spark_batch.value.main_python_file_uri
      jar_file_uris      = spark_batch.value.jar_file_uris
      python_file_uris   = spark_batch.value.python_file_uris
      file_uris          = spark_batch.value.file_uris
      archive_uris       = spark_batch.value.archive_uris
      args               = spark_batch.value.args
    }
  }

  # Runtime Configuration
  dynamic "runtime_config" {
    for_each = [var.runtime_config]
    content {
      version        = runtime_config.value.version
      container_image = runtime_config.value.container_image
      
      dynamic "properties" {
        for_each = runtime_config.value.properties != null ? [runtime_config.value.properties] : []
        content 
          for_each = properties.value
          content {
            key   = each.key
            value = each.value
          }
        }
      }

      dynamic "logging_config" {
        for_each = runtime_config.value.logging_config != null ? [runtime_config.value.logging_config] : []
        content {
          driver_log_levels = logging_config.value.driver_log_levels
        }
      }
    }
  }

  # Environment Configuration
  dynamic "environment_config" {
    for_each = [var.environment_config]
    content {
      dynamic "execution_config" {
        for_each = environment_config.value.execution_config != null ? [environment_config.value.execution_config] : []
        content {
          service_account = execution_config.value.service_account != null ? execution_config.value.service_account : (var.service_account_email != null ? var.service_account_email : google_service_account.dataproc_sa[0].email)
          subnetwork_uri  = execution_config.value.subnetwork_uri != null ? execution_config.value.subnetwork_uri : (length(data.google_compute_subnetwork.default) > 0 ? data.google_compute_subnetwork.default[0].self_link : null)
          network_tags    = execution_config.value.network_tags
          kms_key         = execution_config.value.kms_key
          ttl             = execution_config.value.ttl
          staging_bucket  = execution_config.value.staging_bucket != null ? execution_config.value.staging_bucket : (length(google_storage_bucket.staging_bucket) > 0 ? google_storage_bucket.staging_bucket[0].name : null)
        }
      }

      dynamic "peripherals_config" {
        for_each = environment_config.value.peripherals_config != null ? [environment_config.value.peripherals_config] : []
        content {
          metastore_service = peripherals_config.value.metastore_service
          
          dynamic "spark_history_server_config" {
            for_each = peripherals_config.value.spark_history_server_config != null ? [peripherals_config.value.spark_history_server_config] : []
            content {
              dataproc_cluster = spark_history_server_config.value.dataproc_cluster
            }
          }
        }
      }
    }
  }

  timeouts {
    create = var.timeouts.create
    update = var.timeouts.update
    delete = var.timeouts.delete
  }

  depends_on = [
    google_project_service.apis,
    google_project_iam_member.dataproc_roles,
    google_storage_bucket.staging_bucket
  ]
}
modules/dataproc-serverless/outputs.tf
hcl
output "batch_id" {
  description = "The ID of the Dataproc Serverless batch"
  value       = google_dataproc_batch.spark_batch.batch_id
}

output "batch_name" {
  description = "The full resource name of the batch"
  value       = google_dataproc_batch.spark_batch.name
}

output "batch_uuid" {
  description = "The UUID of the batch"
  value       = google_dataproc_batch.spark_batch.uuid
}

output "state" {
  description = "The state of the batch"
  value       = google_dataproc_batch.spark_batch.state
}

output "create_time" {
  description = "The time when the batch was created"
  value       = google_dataproc_batch.spark_batch.create_time
}

output "runtime_info" {
  description = "Runtime information about the batch execution"
  value       = google_dataproc_batch.spark_batch.runtime_info
}

output "service_account_email" {
  description = "Service account email used for the batch job"
  value       = var.service_account_email != null ? var.service_account_email : (length(google_service_account.dataproc_sa) > 0 ? google_service_account.dataproc_sa[0].email : null)
}

output "staging_bucket" {
  description = "Staging bucket used for the batch job"
  value       = try(var.environment_config.execution_config.staging_bucket, null) != null ? var.environment_config.execution_config.staging_bucket : (length(google_storage_bucket.staging_bucket) > 0 ? google_storage_bucket.staging_bucket[0].name : null)
}

output "output_files" {
  description = "Output files generated by the batch job"
  value       = google_dataproc_batch.spark_batch.runtime_info[*].output_uri
}
Example Usage
examples/basic-spark-job/main.tf
hcl
module "basic_spark_job" {
  source = "../../modules/dataproc-serverless"

  project_id = "my-gcp-project"
  region     = "us-central1"
  batch_id   = "basic-spark-job-001"

  spark_job_config = {
    main_jar_file_uri = "gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
    main_class        = "com.example.SparkETL"
    jar_file_uris = [
      "gs://my-bucket/jars/my-spark-job.jar"
    ]
    args = [
      "--input=gs://my-bucket/input/data",
      "--output=gs://my-bucket/output/results"
    ]
  }

  runtime_config = {
    version = "2.1"
    properties = {
      "spark.sql.adaptive.enabled"             = "true"
      "spark.sql.adaptive.coalescePartitions.enabled" = "true"
      "spark.executor.memory"                  = "4g"
      "spark.driver.memory"                    = "2g"
    }
  }

  environment_config = {
    execution_config = {
      network_tags = ["dataproc-serverless"]
    }
  }

  labels = {
    environment = "dev"
    team        = "data-engineering"
  }
}
examples/advanced-spark-job/main.tf
hcl
module "advanced_spark_job" {
  source = "../../modules/dataproc-serverless"

  project_id = "my-gcp-project"
  region     = "us-central1"
  batch_id   = "advanced-pyspark-job-001"

  spark_job_config = {
    main_python_file_uri = "gs://my-bucket/scripts/main.py"
    python_file_uris = [
      "gs://my-bucket/scripts/utils.py",
      "gs://my-bucket/scripts/transformations.py"
    ]
    file_uris = [
      "gs://my-bucket/config/config.json"
    ]
    archive_uris = [
      "gs://my-bucket/archives/dependencies.zip"
    ]
    args = [
      "--input-table=my-project.dataset.input_table",
      "--output-table=my-project.dataset.output_table",
      "--execution-date=2024-01-01"
    ]
  }

  runtime_config = {
    version = "2.1"
    container_image = "gcr.io/my-project/custom-spark-image:latest"
    properties = {
      "spark.sql.adaptive.enabled"               = "true"
      "spark.sql.adaptive.coalescePartitions.enabled" = "true"
      "spark.executor.memory"                    = "8g"
      "spark.driver.memory"                      = "4g"
      "spark.executor.instances"                 = "10"
      "spark.sql.shuffle.partitions"             = "200"
    }
    logging_config = {
      driver_log_levels = {
        "root" = "INFO"
        "org.apache.spark" = "WARN"
      }
    }
  }

  environment_config = {
    execution_config = {
      subnetwork_uri = "projects/my-project/regions/us-central1/subnetworks/my-subnet"
      network_tags   = ["dataproc-serverless", "pyspark-job"]
      ttl            = "3600s" # 1 hour
    }
    peripherals_config = {
      metastore_service = "projects/my-project/locations/us-central1/services/my-metastore"
    }
  }

  service_account_email = "dataproc-sa@my-project.iam.gserviceaccount.com"

  labels = {
    environment = "production"
    team        = "data-science"
    project     = "customer-analytics"
  }

  timeouts = {
    create = "45m"
    update = "30m"
    delete = "20m"
  }
}
Module Documentation
modules/dataproc-serverless/README.md
markdown
# Terraform Module: GCP Dataproc Serverless for Apache Spark

A reusable Terraform module for deploying serverless Apache Spark batch jobs on Google Cloud Platform using Dataproc Serverless.

## Features

- ðŸš€ Deploy serverless Spark jobs without managing clusters
- âš™ï¸ Configurable Spark properties and runtime settings
- ðŸ” IAM integration with service accounts
- ðŸŒ Network configuration support
- ðŸ—‚ï¸ Automatic staging bucket creation
- ðŸ·ï¸ Labeling and tagging support
- â±ï¸ Configurable timeouts

## Usage

### Basic Example

```hcl
module "spark_job" {
  source = "path/to/module"

  project_id = "my-project"
  region     = "us-central1"
  batch_id   = "my-spark-job"

  spark_job_config = {
    main_jar_file_uri = "gs://my-bucket/my-job.jar"
    main_class        = "com.example.MainClass"
    args              = ["arg1", "arg2"]
  }
}
Advanced Example
hcl
module "advanced_spark_job" {
  source = "path/to/module"

  project_id = "my-project"
  region     = "us-central1"
  batch_id   = "advanced-job"

  spark_job_config = {
    main_python_file_uri = "gs://my-bucket/main.py"
    python_file_uris     = ["gs://my-bucket/utils.py"]
    file_uris            = ["gs://my-bucket/config.json"]
  }

  runtime_config = {
    version = "2.1"
    properties = {
      "spark.executor.memory" = "4g"
      "spark.driver.memory"   = "2g"
    }
  }

  environment_config = {
    execution_config = {
      network_tags = ["spark-job"]
    }
  }
}
Requirements
Terraform >= 1.0

Google Cloud Provider >= 4.0

Inputs
Name	Description	Type	Default
project_id	GCP project ID	string	required
region	GCP region	string	"us-central1"
batch_id	Batch job ID	string	required
spark_job_config	Spark job configuration	object	{}
runtime_config	Runtime configuration	object	{}
environment_config	Environment configuration	object	{}
service_account_email	Service account email	string	null
network_config	Network configuration	object	{}
labels	Resource labels	map(string)	{}
timeouts	Timeout configuration	object	{}
Outputs
Name	Description
batch_id	Batch job ID
batch_name	Full resource name
state	Batch state
service_account_email	Service account used
staging_bucket	Staging bucket name
text

## Key Features

1. **Comprehensive Configuration**: Supports all major Spark job types (JAR, Python, SQL)
2. **Auto-provisioning**: Automatically creates staging buckets and service accounts
3. **Security**: IAM integration and service account management
4. **Networking**: Supports custom VPC networks and subnets
5. **Monitoring**: Integrated logging and monitoring configuration
6. **Cost Control**: Configurable TTL and resource limits
7. **Reusable**: Modular design for different use cases

